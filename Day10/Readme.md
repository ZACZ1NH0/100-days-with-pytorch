## Softmax & Cross Entropy

### Softmax

Softmax chuy·ªÉn c√°c ƒë·∫ßu ra (logits) th√†nh x√°c su·∫•t ƒë·ªÉ m√¥ h√¨nh c√≥ th·ªÉ ch·ªçn l·ªõp.

Softmax bi·∫øn c√°c ƒëi·ªÉm s·ªë (logits) th√†nh x√°c su·∫•t, sao cho:

    T·ªïng x√°c su·∫•t = 1

    L·ªõp n√†o c√≥ ƒëi·ªÉm l·ªõn ‚Üí x√°c su·∫•t cao h∆°n.

C√¥ng th·ª©c:

    Softmax(zi) = e^(zi) / ‚àë(e^zj)

___
V√≠ d·ª•:

B·∫°n c√≥ m·ªôt m√¥ h√¨nh AI, n√≥ ph·∫£i ch·ªçn 1 trong nhi·ªÅu l·ªõp (v√≠ d·ª•: h√¨nh l√† ch√≥, m√®o hay th·ªè?).

M√¥ h√¨nh kh√¥ng n√≥i th·∫≥ng "T√¥i ch·∫Øc ch·∫Øn l√† m√®o", m√† n√≥ s·∫Ω n√≥i b·∫±ng ƒëi·ªÉm s·ªë:
    Ch√≥: 1.2
    M√®o: 3.3
    Th·ªè: 0.5
C√°c con s·ªë n√†y l√† logits (ƒëi·ªÉm ch∆∞a x·ª≠ l√Ω). M√¨nh c·∫ßn bi·∫øn ch√∫ng th√†nh x√°c su·∫•t.

### Cross Entropy Loss l√† g√¨?

Cross Entropy ƒëo m·ª©c ƒë·ªô kh√°c bi·ªát gi·ªØa ph√¢n ph·ªëi x√°c su·∫•t d·ª± ƒëo√°n v√† nh√£n th·∫≠t.

V·ªõi nh√£n th·∫≠t l√† one-hot vector y, v√† ƒë·∫ßu ra d·ª± ƒëo√°n y^

    Loss = -‚àë yi*log(yi^)* 1/N


Gi·∫£ s·ª≠ nh√£n th·∫≠t l√† ‚Äúm√®o‚Äù (class 1). M√† model l·∫°i b·∫£o ‚Äút√¥i nghƒ© 85% l√† m√®o‚Äù, th√¨ 

loss s·∫Ω th·∫•p.

Nh∆∞ng n·∫øu model b·∫£o ‚Äút√¥i nghƒ© 5% l√† m√®o‚Äù, th√¨ loss cao.

Cross Entropy ƒëo m·ª©c ƒë·ªô l·ªách gi·ªØa d·ª± ƒëo√°n v√† th·ª±c t·∫ø.

### V√≠ d·ª• 

Ph√¢n lo·∫°i: Ch√≥, M√®o, G√†

Ta c√≥ m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i 3 l·ªõp:

    L·ªõp 0: Ch√≥

    L·ªõp 1: M√®o

    L·ªõp 2: G√†

[2.0, 1.0, 0.1]  # m·∫´u 1

Sau khi ƒë∆∞a v√†o h√†m softmax, ta nh·∫≠n ƒë∆∞·ª£c:

torch.softmax(torch.tensor([2.0, 1.0, 0.1]), dim=0)

 ‚âà [0.659, 0.242, 0.099]

üëâ M√¥ h√¨nh ‚Äútin‚Äù r·∫±ng:

65.9% l√† Ch√≥

24.2% l√† M√®o

9.9% l√† G√†

=> D·ª± ƒëo√°n nh√£n: Ch√≥ (v√¨ x√°c su·∫•t cao nh·∫•t).


Nh√£n th·∫≠t (target) l√† s·ªë nguy√™n: 0, 1, ho·∫∑c 2

V·ªõi CrossEntropyLoss, kh√¥ng c·∫ßn softmax th·ªß c√¥ng.

Ta ch·ªâ ƒë∆∞a logits th√¥ v√†o, loss s·∫Ω t·ª± t√≠nh n·ªôi b·ªô:

        loss_fn = nn.CrossEntropyLoss()
        logits = torch.tensor([[2.0, 1.0, 0.1]])
        target = torch.tensor([0])
        loss = loss_fn(logits, target)
