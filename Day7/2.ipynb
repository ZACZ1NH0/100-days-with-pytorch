{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da9b72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f56ae579",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c122c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n"
     ]
    }
   ],
   "source": [
    "n_samples, n_features = X.shape # số dòng là số mẫu, số cột là số đặc trưng\n",
    "print(n_samples,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2238f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b88b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51c128f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e39d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuyển y_train từ dạng 1 chiều (1D) sang dạng 2 chiều (2D) có 1 cột và số dòng bằng số mẫu.\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "\n",
    "y_test = y_test.view(y_test.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6220011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c504e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8187d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(n_features)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3321a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:0.6443321704864502\n",
      "epoch:1 loss:0.6277638673782349\n",
      "epoch:2 loss:0.6121492385864258\n",
      "epoch:3 loss:0.5974216461181641\n",
      "epoch:4 loss:0.583518385887146\n",
      "epoch:5 loss:0.5703802704811096\n",
      "epoch:6 loss:0.5579525232315063\n",
      "epoch:7 loss:0.5461838841438293\n",
      "epoch:8 loss:0.5350271463394165\n",
      "epoch:10 loss:0.514378011226654\n",
      "epoch:11 loss:0.5048084259033203\n",
      "epoch:12 loss:0.49569591879844666\n",
      "epoch:13 loss:0.4870092570781708\n",
      "epoch:14 loss:0.47871971130371094\n",
      "epoch:15 loss:0.4708009958267212\n",
      "epoch:16 loss:0.4632289707660675\n",
      "epoch:17 loss:0.4559813439846039\n",
      "epoch:18 loss:0.44903767108917236\n",
      "epoch:20 loss:0.4359881281852722\n",
      "epoch:21 loss:0.42984896898269653\n",
      "epoch:22 loss:0.4239465594291687\n",
      "epoch:23 loss:0.4182674288749695\n",
      "epoch:24 loss:0.4127986431121826\n",
      "epoch:25 loss:0.4075285494327545\n",
      "epoch:26 loss:0.40244626998901367\n",
      "epoch:27 loss:0.3975416123867035\n",
      "epoch:28 loss:0.39280518889427185\n",
      "epoch:30 loss:0.383802205324173\n",
      "epoch:31 loss:0.37951990962028503\n",
      "epoch:32 loss:0.3753740191459656\n",
      "epoch:33 loss:0.3713579773902893\n",
      "epoch:34 loss:0.3674653470516205\n",
      "epoch:35 loss:0.3636903762817383\n",
      "epoch:36 loss:0.36002758145332336\n",
      "epoch:37 loss:0.3564717471599579\n",
      "epoch:38 loss:0.3530180752277374\n",
      "epoch:40 loss:0.3463992476463318\n",
      "epoch:41 loss:0.34322577714920044\n",
      "epoch:42 loss:0.34013769030570984\n",
      "epoch:43 loss:0.33713147044181824\n",
      "epoch:44 loss:0.33420372009277344\n",
      "epoch:45 loss:0.33135125041007996\n",
      "epoch:46 loss:0.3285709321498871\n",
      "epoch:47 loss:0.32585999369621277\n",
      "epoch:48 loss:0.32321566343307495\n",
      "epoch:50 loss:0.3181166648864746\n",
      "epoch:51 loss:0.31565722823143005\n",
      "epoch:52 loss:0.31325483322143555\n",
      "epoch:53 loss:0.31090739369392395\n",
      "epoch:54 loss:0.3086129426956177\n",
      "epoch:55 loss:0.30636951327323914\n",
      "epoch:56 loss:0.30417537689208984\n",
      "epoch:57 loss:0.30202874541282654\n",
      "epoch:58 loss:0.29992803931236267\n",
      "epoch:60 loss:0.29585808515548706\n",
      "epoch:61 loss:0.29388588666915894\n",
      "epoch:62 loss:0.29195380210876465\n",
      "epoch:63 loss:0.290060430765152\n",
      "epoch:64 loss:0.28820452094078064\n",
      "epoch:65 loss:0.2863849103450775\n",
      "epoch:66 loss:0.28460046648979187\n",
      "epoch:67 loss:0.28285008668899536\n",
      "epoch:68 loss:0.28113269805908203\n",
      "epoch:70 loss:0.2777930200099945\n",
      "epoch:71 loss:0.2761687934398651\n",
      "epoch:72 loss:0.27457383275032043\n",
      "epoch:73 loss:0.27300727367401123\n",
      "epoch:74 loss:0.27146822214126587\n",
      "epoch:75 loss:0.26995593309402466\n",
      "epoch:76 loss:0.26846975088119507\n",
      "epoch:77 loss:0.26700878143310547\n",
      "epoch:78 loss:0.2655724287033081\n",
      "epoch:80 loss:0.2627708613872528\n",
      "epoch:81 loss:0.2614043056964874\n",
      "epoch:82 loss:0.26005980372428894\n",
      "epoch:83 loss:0.258736789226532\n",
      "epoch:84 loss:0.2574346959590912\n",
      "epoch:85 loss:0.2561529278755188\n",
      "epoch:86 loss:0.25489097833633423\n",
      "epoch:87 loss:0.25364840030670166\n",
      "epoch:88 loss:0.2524246573448181\n",
      "epoch:90 loss:0.25003185868263245\n",
      "epoch:91 loss:0.24886196851730347\n",
      "epoch:92 loss:0.24770906567573547\n",
      "epoch:93 loss:0.24657288193702698\n",
      "epoch:94 loss:0.24545292556285858\n",
      "epoch:95 loss:0.24434880912303925\n",
      "epoch:96 loss:0.24326026439666748\n",
      "epoch:97 loss:0.24218685925006866\n",
      "epoch:98 loss:0.24112825095653534\n",
      "epoch:100 loss:0.2390540987253189\n",
      "epoch:101 loss:0.23803792893886566\n",
      "epoch:102 loss:0.23703525960445404\n",
      "epoch:103 loss:0.23604576289653778\n",
      "epoch:104 loss:0.23506928980350494\n",
      "epoch:105 loss:0.2341054081916809\n",
      "epoch:106 loss:0.23315392434597015\n",
      "epoch:107 loss:0.2322145402431488\n",
      "epoch:108 loss:0.23128701746463776\n",
      "epoch:110 loss:0.22946660220623016\n",
      "epoch:111 loss:0.22857321798801422\n",
      "epoch:112 loss:0.22769075632095337\n",
      "epoch:113 loss:0.22681894898414612\n",
      "epoch:114 loss:0.22595763206481934\n",
      "epoch:115 loss:0.2251066118478775\n",
      "epoch:116 loss:0.22426560521125793\n",
      "epoch:117 loss:0.2234344780445099\n",
      "epoch:118 loss:0.22261303663253784\n",
      "epoch:120 loss:0.22099840641021729\n",
      "epoch:121 loss:0.22020484507083893\n",
      "epoch:122 loss:0.21942026913166046\n",
      "epoch:123 loss:0.21864446997642517\n",
      "epoch:124 loss:0.21787728369235992\n",
      "epoch:125 loss:0.21711859107017517\n",
      "epoch:126 loss:0.2163681536912918\n",
      "epoch:127 loss:0.21562591195106506\n",
      "epoch:128 loss:0.21489165723323822\n",
      "epoch:130 loss:0.21344661712646484\n",
      "epoch:131 loss:0.21273554861545563\n",
      "epoch:132 loss:0.21203193068504333\n",
      "epoch:133 loss:0.21133559942245483\n",
      "epoch:134 loss:0.21064651012420654\n",
      "epoch:135 loss:0.20996446907520294\n",
      "epoch:136 loss:0.20928941667079926\n",
      "epoch:137 loss:0.20862114429473877\n",
      "epoch:138 loss:0.2079596221446991\n",
      "epoch:140 loss:0.20665624737739563\n",
      "epoch:141 loss:0.2060142308473587\n",
      "epoch:142 loss:0.2053784430027008\n",
      "epoch:143 loss:0.20474885404109955\n",
      "epoch:144 loss:0.20412537455558777\n",
      "epoch:145 loss:0.20350782573223114\n",
      "epoch:146 loss:0.20289622247219086\n",
      "epoch:147 loss:0.2022903710603714\n",
      "epoch:148 loss:0.20169022679328918\n",
      "epoch:150 loss:0.20050668716430664\n",
      "epoch:151 loss:0.19992311298847198\n",
      "epoch:152 loss:0.1993449181318283\n",
      "epoch:153 loss:0.19877196848392487\n",
      "epoch:154 loss:0.19820424914360046\n",
      "epoch:155 loss:0.19764159619808197\n",
      "epoch:156 loss:0.19708400964736938\n",
      "epoch:157 loss:0.19653137028217316\n",
      "epoch:158 loss:0.1959836483001709\n",
      "epoch:160 loss:0.19490253925323486\n",
      "epoch:161 loss:0.19436901807785034\n",
      "epoch:162 loss:0.1938401311635971\n",
      "epoch:163 loss:0.19331581890583038\n",
      "epoch:164 loss:0.19279591739177704\n",
      "epoch:165 loss:0.19228047132492065\n",
      "epoch:166 loss:0.19176937639713287\n",
      "epoch:167 loss:0.19126257300376892\n",
      "epoch:168 loss:0.19075998663902283\n",
      "epoch:170 loss:0.1897672712802887\n",
      "epoch:171 loss:0.1892770379781723\n",
      "epoch:172 loss:0.18879081308841705\n",
      "epoch:173 loss:0.18830853700637817\n",
      "epoch:174 loss:0.1878301203250885\n",
      "epoch:175 loss:0.18735557794570923\n",
      "epoch:176 loss:0.1868848204612732\n",
      "epoch:177 loss:0.18641780316829681\n",
      "epoch:178 loss:0.1859544813632965\n",
      "epoch:180 loss:0.1850387305021286\n",
      "epoch:181 loss:0.18458619713783264\n",
      "epoch:182 loss:0.18413718044757843\n",
      "epoch:183 loss:0.18369160592556\n",
      "epoch:184 loss:0.18324945867061615\n",
      "epoch:185 loss:0.18281064927577972\n",
      "epoch:186 loss:0.18237519264221191\n",
      "epoch:187 loss:0.18194304406642914\n",
      "epoch:188 loss:0.18151411414146423\n",
      "epoch:190 loss:0.18066583573818207\n",
      "epoch:191 loss:0.18024639785289764\n",
      "epoch:192 loss:0.17983005940914154\n",
      "epoch:193 loss:0.17941676080226898\n",
      "epoch:194 loss:0.17900648713111877\n",
      "epoch:195 loss:0.17859919369220734\n",
      "epoch:196 loss:0.1781948208808899\n",
      "epoch:197 loss:0.17779338359832764\n",
      "epoch:198 loss:0.1773947924375534\n",
      "epoch:200 loss:0.17660613358020782\n",
      "epoch:201 loss:0.17621596157550812\n",
      "epoch:202 loss:0.17582853138446808\n",
      "epoch:203 loss:0.17544381320476532\n",
      "epoch:204 loss:0.17506177723407745\n",
      "epoch:205 loss:0.1746823787689209\n",
      "epoch:206 loss:0.17430558800697327\n",
      "epoch:207 loss:0.17393141984939575\n",
      "epoch:208 loss:0.1735597848892212\n",
      "epoch:210 loss:0.17282406985759735\n",
      "epoch:211 loss:0.1724599301815033\n",
      "epoch:212 loss:0.17209824919700623\n",
      "epoch:213 loss:0.17173895239830017\n",
      "epoch:214 loss:0.1713820844888687\n",
      "epoch:215 loss:0.17102757096290588\n",
      "epoch:216 loss:0.1706753671169281\n",
      "epoch:217 loss:0.17032551765441895\n",
      "epoch:218 loss:0.16997793316841125\n",
      "epoch:220 loss:0.16928955912590027\n",
      "epoch:221 loss:0.1689487099647522\n",
      "epoch:222 loss:0.16861005127429962\n",
      "epoch:223 loss:0.16827355325222015\n",
      "epoch:224 loss:0.1679392009973526\n",
      "epoch:225 loss:0.16760699450969696\n",
      "epoch:226 loss:0.16727687418460846\n",
      "epoch:227 loss:0.1669488400220871\n",
      "epoch:228 loss:0.16662286221981049\n",
      "epoch:230 loss:0.16597700119018555\n",
      "epoch:231 loss:0.16565705835819244\n",
      "epoch:232 loss:0.1653391420841217\n",
      "epoch:233 loss:0.16502313315868378\n",
      "epoch:234 loss:0.16470907628536224\n",
      "epoch:235 loss:0.16439694166183472\n",
      "epoch:236 loss:0.1640866994857788\n",
      "epoch:237 loss:0.16377833485603333\n",
      "epoch:238 loss:0.16347184777259827\n",
      "epoch:240 loss:0.1628643423318863\n",
      "epoch:241 loss:0.16256330907344818\n",
      "epoch:242 loss:0.16226407885551453\n",
      "epoch:243 loss:0.16196663677692413\n",
      "epoch:244 loss:0.16167090833187103\n",
      "epoch:245 loss:0.1613769382238388\n",
      "epoch:246 loss:0.16108466684818268\n",
      "epoch:247 loss:0.16079413890838623\n",
      "epoch:248 loss:0.1605052798986435\n",
      "epoch:250 loss:0.15993256866931915\n",
      "epoch:251 loss:0.15964868664741516\n",
      "epoch:252 loss:0.1593664139509201\n",
      "epoch:253 loss:0.15908576548099518\n",
      "epoch:254 loss:0.1588067263364792\n",
      "epoch:255 loss:0.15852923691272736\n",
      "epoch:256 loss:0.15825335681438446\n",
      "epoch:257 loss:0.15797901153564453\n",
      "epoch:258 loss:0.15770617127418518\n",
      "epoch:260 loss:0.15716512501239777\n",
      "epoch:261 loss:0.15689684450626373\n",
      "epoch:262 loss:0.1566300392150879\n",
      "epoch:263 loss:0.15636470913887024\n",
      "epoch:264 loss:0.15610085427761078\n",
      "epoch:265 loss:0.15583841502666473\n",
      "epoch:266 loss:0.15557743608951569\n",
      "epoch:267 loss:0.15531787276268005\n",
      "epoch:268 loss:0.15505969524383545\n",
      "epoch:270 loss:0.1545475274324417\n",
      "epoch:271 loss:0.1542935073375702\n",
      "epoch:272 loss:0.1540408432483673\n",
      "epoch:273 loss:0.15378953516483307\n",
      "epoch:274 loss:0.1535395383834839\n",
      "epoch:275 loss:0.15329086780548096\n",
      "epoch:276 loss:0.15304352343082428\n",
      "epoch:277 loss:0.15279747545719147\n",
      "epoch:278 loss:0.15255272388458252\n",
      "epoch:280 loss:0.15206705033779144\n",
      "epoch:281 loss:0.15182606875896454\n",
      "epoch:282 loss:0.1515863835811615\n",
      "epoch:283 loss:0.15134790539741516\n",
      "epoch:284 loss:0.1511106789112091\n",
      "epoch:285 loss:0.15087464451789856\n",
      "epoch:286 loss:0.1506398320198059\n",
      "epoch:287 loss:0.15040618181228638\n",
      "epoch:288 loss:0.15017375349998474\n",
      "epoch:290 loss:0.14971239864826202\n",
      "epoch:291 loss:0.14948347210884094\n",
      "epoch:292 loss:0.1492556780576706\n",
      "epoch:293 loss:0.14902903139591217\n",
      "epoch:294 loss:0.1488035023212433\n",
      "epoch:295 loss:0.14857912063598633\n",
      "epoch:296 loss:0.14835581183433533\n",
      "epoch:297 loss:0.14813363552093506\n",
      "epoch:298 loss:0.14791256189346313\n",
      "epoch:300 loss:0.14747363328933716\n",
      "epoch:301 loss:0.1472557783126831\n",
      "epoch:302 loss:0.14703895151615143\n",
      "epoch:303 loss:0.1468232274055481\n",
      "epoch:304 loss:0.14660850167274475\n",
      "epoch:305 loss:0.14639486372470856\n",
      "epoch:306 loss:0.14618223905563354\n",
      "epoch:307 loss:0.14597061276435852\n",
      "epoch:308 loss:0.14575999975204468\n",
      "epoch:310 loss:0.14534179866313934\n",
      "epoch:311 loss:0.14513418078422546\n",
      "epoch:312 loss:0.14492753148078918\n",
      "epoch:313 loss:0.1447218805551529\n",
      "epoch:314 loss:0.1445171982049942\n",
      "epoch:315 loss:0.14431345462799072\n",
      "epoch:316 loss:0.14411067962646484\n",
      "epoch:317 loss:0.14390882849693298\n",
      "epoch:318 loss:0.14370793104171753\n",
      "epoch:320 loss:0.14330892264842987\n",
      "epoch:321 loss:0.14311079680919647\n",
      "epoch:322 loss:0.1429135799407959\n",
      "epoch:323 loss:0.14271724224090576\n",
      "epoch:324 loss:0.14252185821533203\n",
      "epoch:325 loss:0.14232730865478516\n",
      "epoch:326 loss:0.1421336680650711\n",
      "epoch:327 loss:0.1419409066438675\n",
      "epoch:328 loss:0.14174902439117432\n",
      "epoch:330 loss:0.1413678228855133\n",
      "epoch:331 loss:0.14117850363254547\n",
      "epoch:332 loss:0.14099003374576569\n",
      "epoch:333 loss:0.14080239832401276\n",
      "epoch:334 loss:0.14061561226844788\n",
      "epoch:335 loss:0.14042966067790985\n",
      "epoch:336 loss:0.1402445137500763\n",
      "epoch:337 loss:0.1400602012872696\n",
      "epoch:338 loss:0.13987666368484497\n",
      "epoch:340 loss:0.1395120620727539\n",
      "epoch:341 loss:0.1393309384584427\n",
      "epoch:342 loss:0.13915061950683594\n",
      "epoch:343 loss:0.13897106051445007\n",
      "epoch:344 loss:0.13879230618476868\n",
      "epoch:345 loss:0.13861434161663055\n",
      "epoch:346 loss:0.13843707740306854\n",
      "epoch:347 loss:0.1382606327533722\n",
      "epoch:348 loss:0.13808491826057434\n",
      "epoch:350 loss:0.1377357542514801\n",
      "epoch:351 loss:0.13756228983402252\n",
      "epoch:352 loss:0.13738954067230225\n",
      "epoch:353 loss:0.13721755146980286\n",
      "epoch:354 loss:0.13704627752304077\n",
      "epoch:355 loss:0.136875718832016\n",
      "epoch:356 loss:0.1367058902978897\n",
      "epoch:357 loss:0.13653676211833954\n",
      "epoch:358 loss:0.13636834919452667\n",
      "epoch:360 loss:0.13603360950946808\n",
      "epoch:361 loss:0.13586728274822235\n",
      "epoch:362 loss:0.13570165634155273\n",
      "epoch:363 loss:0.13553670048713684\n",
      "epoch:364 loss:0.13537241518497467\n",
      "epoch:365 loss:0.1352088302373886\n",
      "epoch:366 loss:0.1350458860397339\n",
      "epoch:367 loss:0.13488364219665527\n",
      "epoch:368 loss:0.1347220242023468\n",
      "epoch:370 loss:0.13440079987049103\n",
      "epoch:371 loss:0.13424116373062134\n",
      "epoch:372 loss:0.13408216834068298\n",
      "epoch:373 loss:0.13392381370067596\n",
      "epoch:374 loss:0.1337660849094391\n",
      "epoch:375 loss:0.13360901176929474\n",
      "epoch:376 loss:0.13345253467559814\n",
      "epoch:377 loss:0.13329671323299408\n",
      "epoch:378 loss:0.13314150273799896\n",
      "epoch:380 loss:0.13283291459083557\n",
      "epoch:381 loss:0.1326795518398285\n",
      "epoch:382 loss:0.13252677023410797\n",
      "epoch:383 loss:0.1323745846748352\n",
      "epoch:384 loss:0.1322230100631714\n",
      "epoch:385 loss:0.13207203149795532\n",
      "epoch:386 loss:0.13192163407802582\n",
      "epoch:387 loss:0.13177184760570526\n",
      "epoch:388 loss:0.13162262737751007\n",
      "epoch:390 loss:0.13132591545581818\n",
      "epoch:391 loss:0.13117842376232147\n",
      "epoch:392 loss:0.13103148341178894\n",
      "epoch:393 loss:0.13088512420654297\n",
      "epoch:394 loss:0.13073931634426117\n",
      "epoch:395 loss:0.13059407472610474\n",
      "epoch:396 loss:0.13044938445091248\n",
      "epoch:397 loss:0.1303052455186844\n",
      "epoch:398 loss:0.13016165792942047\n",
      "epoch:400 loss:0.12987610697746277\n",
      "epoch:401 loss:0.1297341287136078\n",
      "epoch:402 loss:0.12959271669387817\n",
      "epoch:403 loss:0.12945179641246796\n",
      "epoch:404 loss:0.1293114423751831\n",
      "epoch:405 loss:0.12917158007621765\n",
      "epoch:406 loss:0.12903223931789398\n",
      "epoch:407 loss:0.1288934350013733\n",
      "epoch:408 loss:0.12875515222549438\n",
      "epoch:410 loss:0.12848009169101715\n",
      "epoch:411 loss:0.12834332883358002\n",
      "epoch:412 loss:0.12820707261562347\n",
      "epoch:413 loss:0.12807132303714752\n",
      "epoch:414 loss:0.12793606519699097\n",
      "epoch:415 loss:0.12780128419399261\n",
      "epoch:416 loss:0.12766702473163605\n",
      "epoch:417 loss:0.1275332272052765\n",
      "epoch:418 loss:0.12739992141723633\n",
      "epoch:420 loss:0.127134770154953\n",
      "epoch:421 loss:0.12700290977954865\n",
      "epoch:422 loss:0.1268715113401413\n",
      "epoch:423 loss:0.12674061954021454\n",
      "epoch:424 loss:0.1266101896762848\n",
      "epoch:425 loss:0.12648020684719086\n",
      "epoch:426 loss:0.12635070085525513\n",
      "epoch:427 loss:0.1262216418981552\n",
      "epoch:428 loss:0.1260930597782135\n",
      "epoch:430 loss:0.12583725154399872\n",
      "epoch:431 loss:0.12571002542972565\n",
      "epoch:432 loss:0.12558326125144958\n",
      "epoch:433 loss:0.12545691430568695\n",
      "epoch:434 loss:0.12533102929592133\n",
      "epoch:435 loss:0.12520557641983032\n",
      "epoch:436 loss:0.12508057057857513\n",
      "epoch:437 loss:0.12495600432157516\n",
      "epoch:438 loss:0.12483186274766922\n",
      "epoch:440 loss:0.1245848760008812\n",
      "epoch:441 loss:0.12446201592683792\n",
      "epoch:442 loss:0.12433958798646927\n",
      "epoch:443 loss:0.12421759217977524\n",
      "epoch:444 loss:0.12409602105617523\n",
      "epoch:445 loss:0.12397482991218567\n",
      "epoch:446 loss:0.12385407835245132\n",
      "epoch:447 loss:0.12373373657464981\n",
      "epoch:448 loss:0.12361381202936172\n",
      "epoch:450 loss:0.12337515503168106\n",
      "epoch:451 loss:0.12325645983219147\n",
      "epoch:452 loss:0.12313814461231232\n",
      "epoch:453 loss:0.1230202317237854\n",
      "epoch:454 loss:0.12290272116661072\n",
      "epoch:455 loss:0.12278560549020767\n",
      "epoch:456 loss:0.12266886979341507\n",
      "epoch:457 loss:0.12255252152681351\n",
      "epoch:458 loss:0.12243658304214478\n",
      "epoch:460 loss:0.12220584601163864\n",
      "epoch:461 loss:0.12209106236696243\n",
      "epoch:462 loss:0.12197665870189667\n",
      "epoch:463 loss:0.12186259776353836\n",
      "epoch:464 loss:0.12174895405769348\n",
      "epoch:465 loss:0.12163567543029785\n",
      "epoch:466 loss:0.12152275443077087\n",
      "epoch:467 loss:0.12141022086143494\n",
      "epoch:468 loss:0.12129805237054825\n",
      "epoch:470 loss:0.12107480317354202\n",
      "epoch:471 loss:0.12096372991800308\n",
      "epoch:472 loss:0.1208530068397522\n",
      "epoch:473 loss:0.12074265629053116\n",
      "epoch:474 loss:0.12063265591859818\n",
      "epoch:475 loss:0.12052302807569504\n",
      "epoch:476 loss:0.12041372805833817\n",
      "epoch:477 loss:0.12030479311943054\n",
      "epoch:478 loss:0.12019620090723038\n",
      "epoch:480 loss:0.11998006701469421\n",
      "epoch:481 loss:0.11987252533435822\n",
      "epoch:482 loss:0.11976531147956848\n",
      "epoch:483 loss:0.119658462703228\n",
      "epoch:484 loss:0.11955192685127258\n",
      "epoch:485 loss:0.11944574862718582\n",
      "epoch:486 loss:0.11933987587690353\n",
      "epoch:487 loss:0.1192343607544899\n",
      "epoch:488 loss:0.11912918090820312\n",
      "epoch:490 loss:0.11891979724168777\n",
      "epoch:491 loss:0.11881561577320099\n",
      "epoch:492 loss:0.11871171742677689\n",
      "epoch:493 loss:0.11860818415880203\n",
      "epoch:494 loss:0.11850496381521225\n",
      "epoch:495 loss:0.11840205639600754\n",
      "epoch:496 loss:0.1182994619011879\n",
      "epoch:497 loss:0.11819719523191452\n",
      "epoch:498 loss:0.11809524893760681\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_iters):\n",
    "    y_predicted = model(X_train)\n",
    "\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1)%10:\n",
    "        print(f'epoch:{epoch}',f'loss:{loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a666d4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9035087823867798\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
