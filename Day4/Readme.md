# Optimizer

## Optimizer lÃ  gÃ¬?

Optimizer (thuáº­t toÃ¡n tá»‘i Æ°u) lÃ  cÃ´ng cá»¥ giÃºp mÃ´ hÃ¬nh há»c ra cÃ¡c trá»ng sá»‘ (weights vÃ  bias) tá»‘t nháº¥t báº±ng cÃ¡ch giáº£m dáº§n hÃ m máº¥t mÃ¡t (loss function) sau má»—i láº§n láº·p (epoch).

Thay vÃ¬ thá»­ ngáº«u nhiÃªn cÃ¡c trá»ng sá»‘, optimizer Ä‘iá»u chá»‰nh chÃºng dá»±a trÃªn Ä‘áº¡o hÃ m (gradient).

Trong huáº¥n luyá»‡n mÃ´ hÃ¬nh há»c sÃ¢u, optimizer lÃ  má»™t thuáº­t toÃ¡n cÃ³ nhiá»‡m vá»¥ tÃ¬m ra giÃ¡ trá»‹ tá»‘i Æ°u cho trá»ng sá»‘ (weights) vÃ  bias nháº±m giáº£m thiá»ƒu hÃ m máº¥t mÃ¡t (loss function).

Náº¿u khÃ´ng cÃ³ optimizer, chÃºng ta khÃ´ng thá»ƒ cáº­p nháº­t cÃ¡c trá»ng sá»‘ sao cho mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c tá»« dá»¯ liá»‡u. Optimizer sáº½ dá»±a trÃªn gradient (Ä‘áº¡o hÃ m) cá»§a hÃ m máº¥t mÃ¡t Ä‘á»ƒ cáº­p nháº­t trá»ng sá»‘ theo má»™t hÆ°á»›ng giÃºp giáº£m loss.

TÃ³m láº¡i: Optimizer lÃ  cÃ´ng cá»¥ Ä‘á»ƒ â€œhá»câ€ ra trá»ng sá»‘ tá»‘t nháº¥t.

## CÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u phá»• biáº¿n

### Gradient Descent (GD)

Ã tÆ°á»Ÿng:

TÃ¬m cá»±c tiá»ƒu cá»§a hÃ m máº¥t mÃ¡t báº±ng cÃ¡ch cáº­p nháº­t cÃ¡c tham sá»‘ theo hÆ°á»›ng ngÆ°á»£c láº¡i cá»§a Ä‘áº¡o hÃ m.


CÃ´ng thá»©c:
w_new = w_old - learning_rate * gradient

NhÆ°á»£c Ä‘iá»ƒm: cháº­m, tá»‘n tÃ i nguyÃªn vá»›i táº­p dá»¯ liá»‡u lá»›n.

### Stochastic Gradient Descent (SGD)

Stochastic lÃ  1 biáº¿n thá»ƒ cá»§a Gradient Descent . Thay vÃ¬ sau má»—i epoch chÃºng ta sáº½ cáº­p nháº­t trá»ng sá»‘ (Weight) 1 láº§n thÃ¬

trong má»—i epoch cÃ³ N Ä‘iá»ƒm dá»¯ liá»‡u chÃºng ta sáº½ cáº­p nháº­t trá»ng sá»‘ N láº§n. NhÃ¬n vÃ o 1 máº·t , SGD sáº½ lÃ m giáº£m Ä‘i tá»‘c Ä‘á»™ cá»§a
 
1 epoch. Tuy nhiÃªn nhÃ¬n theo 1 hÆ°á»›ng khÃ¡c,SGD sáº½ há»™i tá»¥ ráº¥t nhanh chá»‰ sau vÃ i epoch. CÃ´ng thá»©c SGD cÅ©ng tÆ°Æ¡ng tá»± nhÆ° 
  
GD nhÆ°ng thá»±c hiá»‡n trÃªn tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u.


 
Cáº­p nháº­t trá»ng sá»‘ sau tá»«ng máº«u dá»¯ liá»‡u thay vÃ¬ toÃ n bá»™ táº­p.

Nhanh, phÃ¹ há»£p vá»›i dá»¯ liá»‡u lá»›n vÃ  online learning.

NhÆ°á»£c Ä‘iá»ƒm: cáº­p nháº­t â€œgiáº­t cá»¥câ€, dá»… bá»‹ dao Ä‘á»™ng.

### Momentum

ThÃªm "quÃ¡n tÃ­nh" vÃ o SGD giÃºp trÃ¡nh káº¹t á»Ÿ Ä‘iá»ƒm cá»±c tiá»ƒu cá»¥c bá»™.

CÃ´ng thá»©c:

v = Î³ * v - lr * gradient  
w = w + v
(Î³ thÆ°á»ng lÃ  0.9)

Ã tÆ°á»Ÿng váº­t lÃ½: giá»‘ng nhÆ° viÃªn bi trÆ°á»£t xuá»‘ng dá»‘c, cÃ³ Ä‘Ã  nÃªn vÆ°á»£t qua â€œá»• gÃ â€.

### Adagrad

Tá»± Ä‘iá»u chá»‰nh learning rate cho tá»«ng tham sá»‘, dá»±a trÃªn gradient quÃ¡ khá»©.

GiÃºp há»c tá»‘t hÆ¡n cho cÃ¡c tham sá»‘ hiáº¿m gáº·p.

g += âˆ‡L(w)^2  
w = w - learning_rate / sqrt(g + Îµ) * âˆ‡L(w)

Trong Ä‘Ã³:

g: tá»•ng bÃ¬nh phÆ°Æ¡ng gradient theo tá»«ng tham sá»‘

Îµ: sá»‘ ráº¥t nhá» trÃ¡nh chia cho 0 (thÆ°á»ng lÃ  1e-8)

ğŸ”¸ Æ¯u Ä‘iá»ƒm:
KhÃ´ng cáº§n tuning learning rate

PhÃ¹ há»£p vá»›i dá»¯ liá»‡u rá»i ráº¡c (nhÆ° xá»­ lÃ½ tá»« ngá»¯ - NLP)

ğŸ”¸ NhÆ°á»£c Ä‘iá»ƒm:
Tá»•ng gradient tÄƒng liÃªn tá»¥c â†’ learning rate giáº£m dáº§n vá» gáº§n 0 â†’ mÃ´ hÃ¬nh khÃ´ng há»c Ä‘Æ°á»£c ná»¯a

### RMSprop

Giáº£i quyáº¿t nhÆ°á»£c Ä‘iá»ƒm cá»§a Adagrad báº±ng cÃ¡ch dÃ¹ng trung bÃ¬nh Ä‘á»™ng cá»§a bÃ¬nh phÆ°Æ¡ng gradient, thay vÃ¬ cá»™ng dá»“n mÃ£i.

CÃ´ng thá»©c:

    E[g^2] = Ï * E[g^2] + (1 - Ï) * âˆ‡L(w)^2  

    w = w - learning_rate / sqrt(E[g^2] + Îµ) * âˆ‡L(w)

Trong Ä‘Ã³:

Ï: há»‡ sá»‘ trÆ°á»£t trung bÃ¬nh Ä‘á»™ng (thÆ°á»ng lÃ  0.9)

E[g^2]: giÃ¡ trá»‹ trung bÃ¬nh Ä‘á»™ng cá»§a gradient bÃ¬nh phÆ°Æ¡ng

ğŸ”¸ Æ¯u Ä‘iá»ƒm:

Giá»¯ tá»‘c Ä‘á»™ há»c á»•n Ä‘á»‹nh

Tá»‘t vá»›i dá»¯ liá»‡u cÃ³ gradient thay Ä‘á»•i nhanh

ğŸ”¸ NhÆ°á»£c Ä‘iá»ƒm:

CÃ³ thá»ƒ váº«n káº¹t á»Ÿ local minimum náº¿u gradient khÃ´ng Ä‘á»§ máº¡nh Ä‘á»ƒ vÆ°á»£t qua

### Adam

Káº¿t há»£p Momentum vÃ  RMSprop Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cáº£ hai: vá»«a mÆ°á»£t mÃ  vá»«a thÃ­ch á»©ng Ä‘Æ°á»£c tá»‘c Ä‘á»™ há»c.

Tá»•ng há»£p Æ°u Ä‘iá»ƒm cá»§a Momentum vÃ  RMSprop.

Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh tá»‘c Ä‘á»™ há»c vÃ  hÆ°á»›ng di chuyá»ƒn.

ÄÆ°á»£c dÃ¹ng phá»• biáº¿n nháº¥t hiá»‡n nay vÃ¬ hiá»‡u quáº£ vÃ  á»•n Ä‘á»‹nh.

ğŸ”¸ CÃ´ng thá»©c:

Adam dÃ¹ng 2 giÃ¡ trá»‹ trung bÃ¬nh:

Momentum (m): trung bÃ¬nh Ä‘á»™ng cá»§a gradient

RMS (v): trung bÃ¬nh Ä‘á»™ng cá»§a bÃ¬nh phÆ°Æ¡ng gradient

m_t = Î²1 * m_{t-1} + (1 - Î²1) * g_t  
v_t = Î²2 * v_{t-1} + (1 - Î²2) * g_t^2

m_hat = m_t / (1 - Î²1^t)  
v_hat = v_t / (1 - Î²2^t)  

w = w - learning_rate * m_hat / (sqrt(v_hat) + Îµ)

ğŸ”¸ Æ¯u Ä‘iá»ƒm:

Phá»• biáº¿n nháº¥t trong thá»±c táº¿

Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh learning rate vÃ  giáº£m dao Ä‘á»™ng

Há»™i tá»¥ nhanh, á»•n Ä‘á»‹nh

ğŸ”¸ NhÆ°á»£c Ä‘iá»ƒm:

Cáº§n nhiá»u bá»™ nhá»› hÆ¡n Ä‘á»ƒ lÆ°u tráº¡ng thÃ¡i (momentum + squared gradient)

Náº¿u quÃ¡ phá»¥ thuá»™c cÃ³ thá»ƒ dáº«n Ä‘áº¿n overfitting nháº¹

| Optimizer | Cáº­p nháº­t trÃªn | Learning Rate | Ghi nhá»› quÃ¡ khá»©? | Tá»± Ä‘iá»u chá»‰nh LR? | Dá»… dÃ¹ng? |
| --------- | ------------- | ------------- | ---------------- | ----------------- | -------- |
| GD        | ToÃ n bá»™ táº­p   | Cá»‘ Ä‘á»‹nh       | âŒ                | âŒ                 | âœ…        |
| SGD       | Tá»«ng máº«u      | Cá»‘ Ä‘á»‹nh       | âŒ                | âŒ                 | âœ…        |
| Momentum  | Tá»«ng máº«u      | Cá»‘ Ä‘á»‹nh       | âœ…                | âŒ                 | âœ…        |
| Adagrad   | Tá»«ng máº«u      | Biáº¿n thiÃªn    | âœ… (g^2 cá»™ng dá»“n) | âœ…                 | âœ…        |
| RMSprop   | Tá»«ng máº«u      | Biáº¿n thiÃªn    | âœ… (trung bÃ¬nh)   | âœ…                 | âœ…        |
| Adam      | Tá»«ng máº«u      | Biáº¿n thiÃªn    | âœ… (m + v)        | âœ…                 | âœ…âœ…âœ…   |
