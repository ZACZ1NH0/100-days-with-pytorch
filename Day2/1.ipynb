{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "682b5c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8042a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.,  4.,  4.],\n",
      "        [ 6.,  6.,  6.],\n",
      "        [ 8.,  8.,  8.],\n",
      "        [10., 12., 14.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2.0, 2.0, 2.0], [3.0,3.0,3.0], [4.0,4.0,4.0],[5.0, 6.0, 7.0]], requires_grad=True)# giá trị đưa vào phải là float\n",
    "# requires_grad=True cho phép tính đạo hàm\n",
    "y = x ** 2 \n",
    "y = y.backward(torch.ones_like(x)) # truyền vào một tensor cùng kích thước với x để tính đạo hàm \n",
    "# y là một scalar nên phải truyền vào một tensor cùng kích thước với x\n",
    "# nếu y là một tensor có kích thước khác thì sẽ báo lỗi\n",
    "# với x là một số x = torch.tensor(2.0, requires_grad = True) , y.backward() sẽ không cần truyền vào một tensor cùng kích thước với x\n",
    "# y.backward() sẽ tính đạo hàm của y theo x và lưu vào x.grad\n",
    "print(x.grad)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23d83f",
   "metadata": {},
   "source": [
    "Khi đặt requires_grad=True, PyTorch bắt đầu theo dõi toàn bộ phép toán của tensor đó.\n",
    "Mỗi phép toán sẽ được ghi lại trong một đồ thị ngược (backward graph).\n",
    "Khi gọi y.backward(), PyTorch sẽ lan truyền ngược (backpropagation) và tính đạo hàm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc61f90",
   "metadata": {},
   "source": [
    ".backward() mặc định tính đạo hàm của một giá trị đầu ra duy nhất (thường là loss trong mạng nơ-ron), và truyền gradient ngược qua từng phép toán đã thực hiện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# z = w*x + b\n",
    "z = w * x + b\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)  \n",
    "print(w.grad)  \n",
    "print(b.grad)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f765e",
   "metadata": {},
   "source": [
    "## Xóa gradient cũ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9057c1",
   "metadata": {},
   "source": [
    "Trong PyTorch, khi gọi .backward() nhiều lần, các giá trị gradient sẽ bị cộng dồn (accumulate) thay vì được ghi đè."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f96069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.)\n",
      "tensor(112.)\n",
      "tensor(114.)\n"
     ]
    }
   ],
   "source": [
    "# x.grad.zero_()  # Đặt lại đạo hàm về 0\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "z = w * x**2 + b\n",
    "z.backward()\n",
    "print(x.grad)  # In ra đạo hàm mới của x\n",
    "z = w * x**3 + b\n",
    "z.backward()\n",
    "print(x.grad)  # In ra đạo hàm mới của x, sẽ cộng dồn với đạo hàm trước đó\n",
    "z = w * x\n",
    "z.backward()\n",
    "print(x.grad)  # In ra đạo hàm mới của x, sẽ cộng dồn với đạo hàm trước đó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d38690",
   "metadata": {},
   "source": [
    "Để tránh cộng dồn ta xóa gradient cũ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "229ad2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.)\n",
      "tensor(96.)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()  # Đặt lại đạo hàm về 0\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "z = w * x**2 + b\n",
    "z.backward()\n",
    "print(x.grad)  # In ra đạo hàm mới của x\n",
    "\n",
    "x.grad.zero_()  # Đặt lại đạo hàm về 0\n",
    "z = w * x**3 + b\n",
    "z.backward()\n",
    "print(x.grad)  # In ra đạo hàm mới của x, sẽ cộng dồn với đạo hàm trước đó\n",
    "\n",
    "x.grad.zero_()  # Đặt lại đạo hàm về 0\n",
    "z = w * x\n",
    "z.backward()\n",
    "print(x.grad)  # In ra đạo hàm mới của x, sẽ cộng dồn với đạo hàm trước đó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea6d86",
   "metadata": {},
   "source": [
    "### Retain_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ab9fc",
   "metadata": {},
   "source": [
    "Có thể sử dụng phương thức `retain_graph=True` trong hàm `backward()`. Điều này cho phép tính đạo hàm nhiều lần mà không cần phải đặt lại đồ thị tính toán. Dưới đây là ví dụ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20714548",
   "metadata": {},
   "source": [
    "Khi không dùng retain_graph = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)  \u001b[38;5;66;03m# In ra đạo hàm của y theo x\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Workspace\\python\\Pytorch\\venv\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Workspace\\python\\Pytorch\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Workspace\\python\\Pytorch\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print(x.grad)  # In ra đạo hàm của y theo x\n",
    "\n",
    "y.backward() \n",
    "# gọi lại backward sẽ báo lỗi vì x.grad đã được tính và không thể tính đạo hàm lại\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62c3b9",
   "metadata": {},
   "source": [
    "Khi gọi retain_graph = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81635d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad lần 1: tensor(4.)\n",
      "grad lần 2: tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "y.backward(retain_graph=True)  # OK\n",
    "print(\"grad lần 1:\", x.grad)   # 8.0\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward()  # OK, vì retain_graph ở trên\n",
    "print(\"grad lần 2:\", x.grad)  # 8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b439612",
   "metadata": {},
   "source": [
    "Tính đạo hàm bậc 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56019823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: 27.0\n",
      "d2y/dx2: 18.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x ** 3  \n",
    "# đạo hàm cấp 1\n",
    "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0] \n",
    "\n",
    "# đạo hàm cấp 2\n",
    "d2y_dx2 = torch.autograd.grad(dy_dx, x)[0]  \n",
    "\n",
    "print(\"dy/dx:\", dy_dx.item())      \n",
    "print(\"d2y/dx2:\", d2y_dx2.item())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027f7c5",
   "metadata": {},
   "source": [
    "| Tính năng           | `retain_graph=True`                             | `x.grad.zero_()`                          |\n",
    "| ------------------- | ----------------------------------------------- | ----------------------------------------- |\n",
    "| **Liên quan đến**   | Đồ thị tính toán (computation graph)            | Gradient lưu trong `x.grad`               |\n",
    "| **Mục đích chính**  | Cho phép `.backward()` chạy **lại**             | Reset gradient về **0**                   |\n",
    "| **Dùng khi nào?**   | Cần tính đạo hàm nhiều lần **trên cùng đồ thị** | Trước mỗi batch hoặc mỗi lần `backward()` |\n",
    "| **Nếu không dùng?** | `.backward()` lần 2 sẽ báo lỗi                  | Gradient sẽ bị **cộng dồn**               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd583057",
   "metadata": {},
   "source": [
    "| Thuộc tính                  | `.backward()`              | `torch.autograd.grad()`             |\n",
    "| --------------------------- | -------------------------- | ----------------------------------- |\n",
    "| Trả về kết quả?             | ❌ Không (cập nhật `.grad`) | ✅ Có (trả về gradient)              |\n",
    "| Có ghi vào `.grad`?         | ✅ Có                       | ❌ Không                             |\n",
    "| Dùng trong đạo hàm bậc 2?   | ❌ Không tốt lắm            | ✅ Có thể tạo graph để đạo hàm bậc 2 |\n",
    "| Dễ điều khiển hơn cho grad? | ❌ Ít linh hoạt             | ✅ Linh hoạt và chính xác hơn        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16d9f4",
   "metadata": {},
   "source": [
    "Khi nào dùng autograd\n",
    "| Trường hợp                                         | Dùng cái nào?                      |\n",
    "| -------------------------------------------------- | ---------------------------------- |\n",
    "| Huấn luyện bình thường                             | `.backward()`                      |\n",
    "| Muốn lấy đạo hàm **mà không làm bẩn .grad**        | `autograd.grad()`                  |\n",
    "| Tính đạo hàm bậc hai (gradient của gradient)       | `autograd.grad(create_graph=True)` |\n",
    "| Tính grad theo cách **tùy biến (vector-Jacobian)** | `autograd.grad()`                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c83cb",
   "metadata": {},
   "source": [
    "# Loss func cơ bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9198de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0\n",
      "Gradient of x: -2.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, dtype=torch.float32)\n",
    "loss = (x - y) ** 2\n",
    "loss.backward()  # Tính đạo hàm của loss theo x\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Gradient of x:\", x.grad.item())  # In ra đạo hàm của loss theo x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
